<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scrapy初上手: 爬取小说记录 | 烂杯的博客</title><meta name=keywords content="python,scrapy"><meta name=description content="开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.
准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 pip install scrapy 即可.
编译环境 编译环境当然还是永远的 VS Code.
实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.
那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:
 根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 ."><meta name=author content="cgcel"><link rel=canonical href=https://cgcel.github.io/posts/2022/05/17/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://cgcel.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://cgcel.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://cgcel.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://cgcel.github.io/images/avatar.jpg><link rel=mask-icon href=https://cgcel.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Scrapy初上手: 爬取小说记录"><meta property="og:description" content="开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.
准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 pip install scrapy 即可.
编译环境 编译环境当然还是永远的 VS Code.
实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.
那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:
 根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 ."><meta property="og:type" content="article"><meta property="og:url" content="https://cgcel.github.io/posts/2022/05/17/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/"><meta property="og:image" content="https://cgcel.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-17T14:13:36+00:00"><meta property="article:modified_time" content="2022-05-17T14:13:36+00:00"><meta property="og:site_name" content="CGCEL BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cgcel.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Scrapy初上手: 爬取小说记录"><meta name=twitter:description content="开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.
准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 pip install scrapy 即可.
编译环境 编译环境当然还是永远的 VS Code.
实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.
那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:
 根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://cgcel.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Scrapy初上手: 爬取小说记录","item":"https://cgcel.github.io/posts/2022/05/17/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scrapy初上手: 爬取小说记录","name":"Scrapy初上手: 爬取小说记录","description":"开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.\n准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 pip install scrapy 即可.\n编译环境 编译环境当然还是永远的 VS Code.\n实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.\n那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:\n 根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 .","keywords":["python","scrapy"],"articleBody":"开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.\n准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 pip install scrapy 即可.\n编译环境 编译环境当然还是永远的 VS Code.\n实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.\n那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:\n 根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 .txt 文件中  开始 首先, 安装完 Scrapy 后, 按照官方文档, 直接在目录启动命令行, 捅过命令行生成项目:\n1  $ scrapy startproject biquge_dl   生成后, 在 spiders/ 子文件夹中新建 fiction_spider.py, 用于编写爬虫代码.\n根据官方的 QuickStart 例程, 首先新建一个 class, 继承 scrapy.Spider, 后续只需要结合源码和文档, 就可以继续写下去了.\n获取小说章节 URL 首先在浏览器打开开发者工具对小说主页进行抓包, 可以看到该网站并没有用到一些 API, 于是准备采用 XPATH 来对章节 URL 进行爬取, 观察到无论多长篇幅的小说, 其章节都在一页中显示, 这无疑降低了部分难度, 拿来练手 scrapy 实在是再合适不过了.\n通过重写 parse(), 对小说主页 html 进行解析并抓取章节 URL.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import scrapy class FictionSpider(scrapy.Spider): def parse(self, response, **kwargs): self.logger.info('Parse function called on %s', response.url) fiction_name = response.xpath('//*[@id=\"info\"]/h1/text()').get() section_order = 1 for episode_doc in response.xpath('//*[@id=\"list\"]/dl/dd[*]/a'): section_title = episode_doc.xpath('.//text()').get() url = episode_doc.xpath('.//@href').get() section_url = self.base_url + url request = scrapy.Request( url=section_url, callback=self.parse_content, cb_kwargs=dict()) request.cb_kwargs['fiction_name'] = fiction_name request.cb_kwargs['section_order'] = section_order request.cb_kwargs['section_title'] = section_title section_order += 1 yield request   获取成功后, 将小说名, 章节序号, 章节名作为参数传给 parse_content() 函数, 继而对章节进行爬取.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from bs4 import BeautifulSoup as bs from biquge_dl.items import BiqugeDlItem class FictionSpider(scrapy.Spider): def parse_content(self, response, fiction_name, section_order, section_title): self.logger.info('Parse content function called on %s', response.url) contents = response.xpath('//*[@id=\"content\"]/text()').getall() if len(contents)  1: soup = bs(response.body, 'html.parser') contents = soup.find_all(\"div\", {\"class\": \"box_con\"}).get_text() item = BiqugeDlItem() item['name'] = fiction_name item['order'] = section_order item['title'] = section_title item['content'] = contents return item   Items 的使用 写上述 parse_content() 时发现, 直接使用scrapy对小说进行章节爬取, 并按章节名每章下载是很简单的, 只需要直接跑爬虫就可以, 各个章节会乱序下载下来, 但如果想要只下载一份文件, 按顺序存储章节, 那么就需要使用 scrapy 的 items 和 pipelines 了, 这两个 .py 文件会随着一开始命令行创建项目时一并创建, 所以可以很轻易地在 biquge_dl/ 文件夹内找到, 只需要对这两个文件进行重写, 并在 parse_content() 中将内容写进 item 中, 就能实现参数的传递.\n查阅 官方文档, 对 Items 的描述如下:\nThe main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Spiders may return the extracted data as items, Python objects that define key-value pairs.\nScrapy supports multiple types of items. When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should work for any item type.\n可见 item 可以将提取的数据按一定格式返回, 只需要规划好传入参数, 就可以利用 item 实现排序的操作.\nItem pipeline 的使用 官方文档 这样介绍:\nAfter an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.\nEach item pipeline component (sometimes referred as just “Item Pipeline”) is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.\nTypical uses of item pipelines are:\n cleansing HTML data validating scraped data (checking that the items contain certain fields) checking for duplicates (and dropping them) storing the scraped item in a database  意为, 在 spider 抓取一个 item 后, scrapy 通过 Item Pipeline 来处理这个 item, 结合本项目, 只需要将爬取到的 item 交由 Item Pipeline 统一处理即可实现按顺序存储的功能.\n传递章节参数, 实现按顺序保存 为了实现章节按照顺序写入, 首先确认实现思路:\n 在 items.py 定义一个 Item 类, 自定义传入参数, 以供调用 在 pipelines.py 定义一个 Pipeline 类, 对传入的 item 进行处理  items.py 打开 items.py, 按照例子创建一个继承 scrapy.Item 的 BiqugeDlItem 类, 对类进行参数的定义:\n1 2 3 4 5 6 7 8  class BiqugeDlItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() name = scrapy.Field() order = scrapy.Field() title = scrapy.Field() content = scrapy.Field()   可见, 这里定义了几个参数, 分别是:\n name: 小说名 order: 章节序号 title: 章节标题 content: 章节内容  此时再回看 parse_content() 函数, 函数实例化了一个 BiqugeDlItem 类, 并将小说名, 章节序号, 章节标题以及爬取得到的章节内容存入了 item 中, 最终将存有单章节数据的 item 对象返回.\npipelines.py 打开 pipelines.py, 定义一个 BiqugeDlPipeline 类, 在类中重写 open_spider(self, spider), process_item(self, item, spider) 以及 close_spider(self, spider) 函数, 根据文档介绍:\n open_spider(self, spider) 函数在爬虫启动时被调用 process_item(self, item, spider) 函数处理每一个返回的 item 对象 close_spider(self, spider) 函数在爬虫运行结束后调用  结合以上特点, 我们可以在 pipielines.py 中实现对包含小说名, 章节序号, 章节标题, 章节内容的 item 的处理, 代码如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class BiqugeDlPipeline: def open_spider(self, spider): self.items = [] def process_item(self, item, spider): self.items.append(item) return item def close_spider(self, spider): self.items.sort(key=lambda i: i['order']) for item in self.items: # print(item['title']) with open('downloads/{}.txt'.format(item['name']), 'a', encoding='utf-8') as f: f.write('\\n{}\\n'.format(item['title'])) for content in item['content']: f.write(content.strip())   在 open_spider() 中, 先定义一个空列表, 用于存储各个 item, 在 process_item() 中, 将传入的 item 添加至 items 列表中, 在 close_spider() 中, 对 items 列表进行排序操作, 然后将列表中内容写入最终文件, 实现了小说的按序存储. 至此, 整个爬取流程结束.\n运行爬虫 调试完成后, 在命令行启动爬虫, 测试爬取效果:\n1  $ scrapy crawl fiction_spider -a url=https://www.biduoxs.com/biquge/79_79244/   总结 由于疫情管控在家荒废了一段时间, 不过好在还算有一点残存的自制力, 努力学习了一点东西, 这篇博客应该算是手敲的最长的一篇了, 虽然花的时间长了一点, 但是总归是学到了一些, 特别是结合源码和文档来边总结边学习, 能够领会到一些新知识. 这是对着例程敲代码所比较难学到的.\n","wordCount":"748","inLanguage":"en","datePublished":"2022-05-17T14:13:36Z","dateModified":"2022-05-17T14:13:36Z","author":{"@type":"Person","name":"cgcel"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cgcel.github.io/posts/2022/05/17/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/"},"publisher":{"@type":"Organization","name":"烂杯的博客","logo":{"@type":"ImageObject","url":"https://cgcel.github.io/images/avatar.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cgcel.github.io accesskey=h title="烂杯的博客 (Alt + H)"><img src=https://cgcel.github.io/images/avatar.jpg alt=logo aria-label=logo height=35>烂杯的博客</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://cgcel.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://cgcel.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://cgcel.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cgcel.github.io>Home</a>&nbsp;»&nbsp;<a href=https://cgcel.github.io/posts/>Posts</a></div><h1 class=post-title>Scrapy初上手: 爬取小说记录</h1><div class=post-meta><span title="2022-05-17 14:13:36 +0000 UTC">May 17, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;cgcel&nbsp;|&nbsp;<a href=https://github.com/cgcel.github.io/content/posts/2022/05/17/Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b-%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%80%e7%af%87 aria-label=开篇>开篇</a></li><li><a href=#%e5%87%86%e5%a4%87 aria-label=准备>准备</a><ul><li><a href=#%e5%ae%89%e8%a3%85 aria-label=安装>安装</a></li><li><a href=#%e7%bc%96%e8%af%91%e7%8e%af%e5%a2%83 aria-label=编译环境>编译环境</a></li></ul></li><li><a href=#%e5%ae%9e%e7%8e%b0 aria-label=实现>实现</a><ul><li><a href=#%e8%ae%a1%e5%88%92 aria-label=计划>计划</a></li><li><a href=#%e5%bc%80%e5%a7%8b aria-label=开始>开始</a></li><li><a href=#%e8%8e%b7%e5%8f%96%e5%b0%8f%e8%af%b4%e7%ab%a0%e8%8a%82-url aria-label="获取小说章节 URL">获取小说章节 URL</a></li><li><a href=#items-%e7%9a%84%e4%bd%bf%e7%94%a8 aria-label="Items 的使用">Items 的使用</a></li><li><a href=#item-pipeline-%e7%9a%84%e4%bd%bf%e7%94%a8 aria-label="Item pipeline 的使用">Item pipeline 的使用</a></li><li><a href=#%e4%bc%a0%e9%80%92%e7%ab%a0%e8%8a%82%e5%8f%82%e6%95%b0-%e5%ae%9e%e7%8e%b0%e6%8c%89%e9%a1%ba%e5%ba%8f%e4%bf%9d%e5%ad%98 aria-label="传递章节参数, 实现按顺序保存">传递章节参数, 实现按顺序保存</a><ul><li><a href=#itemspy aria-label=items.py>items.py</a></li><li><a href=#pipelinespy aria-label=pipelines.py>pipelines.py</a></li></ul></li></ul></li><li><a href=#%e8%bf%90%e8%a1%8c%e7%88%ac%e8%99%ab aria-label=运行爬虫>运行爬虫</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></div></details></div><div class=post-content><h2 id=开篇>开篇<a hidden class=anchor aria-hidden=true href=#开篇>#</a></h2><p>由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.</p><h2 id=准备>准备<a hidden class=anchor aria-hidden=true href=#准备>#</a></h2><h3 id=安装>安装<a hidden class=anchor aria-hidden=true href=#安装>#</a></h3><p>现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用用 pip 安装. 现在只需要一句 <code>pip install scrapy</code> 即可.</p><h3 id=编译环境>编译环境<a hidden class=anchor aria-hidden=true href=#编译环境>#</a></h3><p>编译环境当然还是永远的 VS Code.</p><h2 id=实现>实现<a hidden class=anchor aria-hidden=true href=#实现>#</a></h2><h3 id=计划>计划<a hidden class=anchor aria-hidden=true href=#计划>#</a></h3><p>大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.</p><p>那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:</p><ul><li>根据小说主页 URL 获取各章节标题以及 URL</li><li>根据章节 URL 爬取章节内容并保存</li><li>将 Scrapy 爬取到的内容按顺序排列后, 依次存入 <code>.txt</code> 文件中</li></ul><h3 id=开始>开始<a hidden class=anchor aria-hidden=true href=#开始>#</a></h3><p>首先, 安装完 Scrapy 后, 按照官方文档, 直接在目录启动命令行, 捅过命令行生成项目:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$ scrapy startproject biquge_dl
</span></span></code></pre></td></tr></table></div></div><p>生成后, 在 <code>spiders/</code> 子文件夹中新建 <code>fiction_spider.py</code>, 用于编写爬虫代码.</p><p>根据官方的 QuickStart 例程, 首先新建一个 class, 继承 <code>scrapy.Spider</code>, 后续只需要结合源码和文档, 就可以继续写下去了.</p><h3 id=获取小说章节-url>获取小说章节 URL<a hidden class=anchor aria-hidden=true href=#获取小说章节-url>#</a></h3><p>首先在浏览器打开开发者工具对小说主页进行抓包, 可以看到该网站并没有用到一些 API, 于是准备采用 XPATH 来对章节 URL 进行爬取, 观察到无论多长篇幅的小说, 其章节都在一页中显示, 这无疑降低了部分难度, 拿来练手 scrapy 实在是再合适不过了.</p><p><img loading=lazy src=Pasted_image_20220524212212.png alt></p><p>通过重写 <code>parse()</code>, 对小说主页 html 进行解析并抓取章节 URL.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FictionSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s1>&#39;Parse function called on </span><span class=si>%s</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>response</span><span class=o>.</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fiction_name</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;info&#34;]/h1/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>section_order</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>episode_doc</span> <span class=ow>in</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/dl/dd[*]/a&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>section_title</span> <span class=o>=</span> <span class=n>episode_doc</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;.//text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>            
</span></span><span class=line><span class=cl>            <span class=n>url</span> <span class=o>=</span> <span class=n>episode_doc</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;.//@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>            
</span></span><span class=line><span class=cl>            <span class=n>section_url</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>base_url</span> <span class=o>+</span> <span class=n>url</span>            
</span></span><span class=line><span class=cl>            <span class=n>request</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>url</span><span class=o>=</span><span class=n>section_url</span><span class=p>,</span> <span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse_content</span><span class=p>,</span> <span class=n>cb_kwargs</span><span class=o>=</span><span class=nb>dict</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                
</span></span><span class=line><span class=cl>            <span class=n>request</span><span class=o>.</span><span class=n>cb_kwargs</span><span class=p>[</span><span class=s1>&#39;fiction_name&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>fiction_name</span>            
</span></span><span class=line><span class=cl>            <span class=n>request</span><span class=o>.</span><span class=n>cb_kwargs</span><span class=p>[</span><span class=s1>&#39;section_order&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>section_order</span>            
</span></span><span class=line><span class=cl>            <span class=n>request</span><span class=o>.</span><span class=n>cb_kwargs</span><span class=p>[</span><span class=s1>&#39;section_title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>section_title</span>            
</span></span><span class=line><span class=cl>            <span class=n>section_order</span> <span class=o>+=</span> <span class=mi>1</span>            
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=n>request</span>
</span></span></code></pre></td></tr></table></div></div><p>获取成功后, 将小说名, 章节序号, 章节名作为参数传给 <code>parse_content()</code> 函数, 继而对章节进行爬取.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span> <span class=k>as</span> <span class=n>bs</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>biquge_dl.items</span> <span class=kn>import</span> <span class=n>BiqugeDlItem</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FictionSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parse_content</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=n>fiction_name</span><span class=p>,</span> <span class=n>section_order</span><span class=p>,</span> <span class=n>section_title</span><span class=p>):</span>   
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s1>&#39;Parse content function called on </span><span class=si>%s</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>response</span><span class=o>.</span><span class=n>url</span><span class=p>)</span>       
</span></span><span class=line><span class=cl>        <span class=n>contents</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;content&#34;]/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>getall</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>contents</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>1</span><span class=p>:</span>       
</span></span><span class=line><span class=cl>            <span class=n>soup</span> <span class=o>=</span> <span class=n>bs</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>body</span><span class=p>,</span> <span class=s1>&#39;html.parser&#39;</span><span class=p>)</span>           
</span></span><span class=line><span class=cl>            <span class=n>contents</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s2>&#34;div&#34;</span><span class=p>,</span> <span class=p>{</span><span class=s2>&#34;class&#34;</span><span class=p>:</span> <span class=s2>&#34;box_con&#34;</span><span class=p>})</span><span class=o>.</span><span class=n>get_text</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=n>item</span> <span class=o>=</span> <span class=n>BiqugeDlItem</span><span class=p>()</span>        
</span></span><span class=line><span class=cl>        <span class=n>item</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>fiction_name</span>        
</span></span><span class=line><span class=cl>        <span class=n>item</span><span class=p>[</span><span class=s1>&#39;order&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>section_order</span>        
</span></span><span class=line><span class=cl>        <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>section_title</span>
</span></span><span class=line><span class=cl>        <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>contents</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>item</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=items-的使用>Items 的使用<a hidden class=anchor aria-hidden=true href=#items-的使用>#</a></h3><p>写上述 <code>parse_content()</code> 时发现, 直接使用scrapy对小说进行章节爬取, 并按章节名每章下载是很简单的, 只需要直接跑爬虫就可以, 各个章节会乱序下载下来, 但如果想要只下载一份文件, 按顺序存储章节, 那么就需要使用 scrapy 的 <code>items</code> 和 <code>pipelines</code> 了, 这两个 .py 文件会随着一开始命令行创建项目时一并创建, 所以可以很轻易地在 <code>biquge_dl/</code> 文件夹内找到, 只需要对这两个文件进行重写, 并在 <code>parse_content()</code> 中将内容写进 item 中, 就能实现参数的传递.</p><p>查阅 <a href=%5Bhttps://docs.scrapy.org/en/latest/topics/items.html%5D(https://docs.scrapy.org/en/latest/topics/items.html#module-scrapy.item)>官方文档</a>, 对 Items 的描述如下:</p><p><em>The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. <a href=https://docs.scrapy.org/en/latest/topics/spiders.html#topics-spiders>Spiders</a> may return the extracted data as items, Python objects that define key-value pairs.</em></p><p><em>Scrapy supports <a href=https://docs.scrapy.org/en/latest/topics/items.html#item-types>multiple types of items</a>. When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should <a href=https://docs.scrapy.org/en/latest/topics/items.html#supporting-item-types>work for any item type</a>.</em></p><p>可见 item 可以将提取的数据按一定格式返回, 只需要规划好传入参数, 就可以利用 item 实现排序的操作.</p><h3 id=item-pipeline-的使用>Item pipeline 的使用<a hidden class=anchor aria-hidden=true href=#item-pipeline-的使用>#</a></h3><p><a href=https://docs.scrapy.org/en/latest/topics/item-pipeline.html>官方文档</a> 这样介绍:</p><p><em>After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.</em></p><p><em>Each item pipeline component (sometimes referred as just “Item Pipeline”) is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.</em></p><p><em>Typical uses of item pipelines are:</em></p><ul><li><em>cleansing HTML data</em></li><li><em>validating scraped data (checking that the items contain certain fields)</em></li><li><em>checking for duplicates (and dropping them)</em></li><li><em>storing the scraped item in a database</em></li></ul><p>意为, 在 spider 抓取一个 item 后, scrapy 通过 Item Pipeline 来处理这个 item, 结合本项目, 只需要将爬取到的 item 交由 Item Pipeline 统一处理即可实现按顺序存储的功能.</p><h3 id=传递章节参数-实现按顺序保存>传递章节参数, 实现按顺序保存<a hidden class=anchor aria-hidden=true href=#传递章节参数-实现按顺序保存>#</a></h3><p>为了实现章节按照顺序写入, 首先确认实现思路:</p><ol><li>在 items.py 定义一个 Item 类, 自定义传入参数, 以供调用</li><li>在 pipelines.py 定义一个 Pipeline 类, 对传入的 item 进行处理</li></ol><h4 id=itemspy>items.py<a hidden class=anchor aria-hidden=true href=#itemspy>#</a></h4><p>打开 items.py, 按照例子创建一个继承 <code>scrapy.Item</code> 的 <code>BiqugeDlItem</code> 类, 对类进行参数的定义:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BiqugeDlItem</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Item</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># define the fields for your item here like:</span>
</span></span><span class=line><span class=cl>    <span class=c1># name = scrapy.Field()</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>order</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>content</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>可见, 这里定义了几个参数, 分别是:</p><ul><li>name: 小说名</li><li>order: 章节序号</li><li>title: 章节标题</li><li>content: 章节内容</li></ul><p>此时再回看 <code>parse_content()</code> 函数, 函数实例化了一个 BiqugeDlItem 类, 并将小说名, 章节序号, 章节标题以及爬取得到的章节内容存入了 item 中, 最终将存有单章节数据的 item 对象返回.</p><h4 id=pipelinespy>pipelines.py<a hidden class=anchor aria-hidden=true href=#pipelinespy>#</a></h4><p>打开 pipelines.py, 定义一个 <code>BiqugeDlPipeline</code> 类, 在类中重写 <code>open_spider(self, spider)</code>, <code>process_item(self, item, spider)</code> 以及 <code>close_spider(self, spider)</code> 函数, 根据文档介绍:</p><ul><li><code>open_spider(self, spider)</code> 函数在爬虫启动时被调用</li><li><code>process_item(self, item, spider)</code> 函数处理每一个返回的 item 对象</li><li><code>close_spider(self, spider)</code> 函数在爬虫运行结束后调用</li></ul><p>结合以上特点, 我们可以在 pipielines.py 中实现对包含小说名, 章节序号, 章节标题, 章节内容的 item 的处理, 代码如下:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BiqugeDlPipeline</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>open_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>items</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>items</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>item</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>close_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>items</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>i</span><span class=p>[</span><span class=s1>&#39;order&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>items</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># print(item[&#39;title&#39;])</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;downloads/</span><span class=si>{}</span><span class=s1>.txt&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>]),</span> <span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=si>{}</span><span class=se>\n</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>content</span> <span class=ow>in</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>content</span><span class=o>.</span><span class=n>strip</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>在 <code>open_spider()</code> 中, 先定义一个空列表, 用于存储各个 item, 在 <code>process_item()</code> 中, 将传入的 item 添加至 items 列表中, 在 <code>close_spider()</code> 中, 对 items 列表进行排序操作, 然后将列表中内容写入最终文件, 实现了小说的按序存储. 至此, 整个爬取流程结束.</p><h2 id=运行爬虫>运行爬虫<a hidden class=anchor aria-hidden=true href=#运行爬虫>#</a></h2><p>调试完成后, 在命令行启动爬虫, 测试爬取效果:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ scrapy crawl fiction_spider -a <span class=nv>url</span><span class=o>=</span>https://www.biduoxs.com/biquge/79_79244/
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=Pasted_image_20220525175633.png alt></p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>由于疫情管控在家荒废了一段时间, 不过好在还算有一点残存的自制力, 努力学习了一点东西, 这篇博客应该算是手敲的最长的一篇了, 虽然花的时间长了一点, 但是总归是学到了一些, 特别是结合源码和文档来边总结边学习, 能够领会到一些新知识. 这是对着例程敲代码所比较难学到的.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://cgcel.github.io/tags/python/>python</a></li><li><a href=https://cgcel.github.io/tags/scrapy/>scrapy</a></li></ul><nav class=paginav><a class=next href=https://cgcel.github.io/posts/2022/04/13/%E4%B8%80%E5%8A%A08t%E5%88%B7nameless-aosp%E8%AE%B0%E5%BD%95/><span class=title>Next »</span><br><span>一加8T刷Nameless AOSP记录</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on twitter" href="https://twitter.com/intent/tweet/?text=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95&url=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f&hashtags=python%2cscrapy"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f&title=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95&summary=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95&source=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f&title=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on whatsapp" href="https://api.whatsapp.com/send?text=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95%20-%20https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy初上手: 爬取小说记录 on telegram" href="https://telegram.me/share/url?text=Scrapy%e5%88%9d%e4%b8%8a%e6%89%8b%3a%20%e7%88%ac%e5%8f%96%e5%b0%8f%e8%af%b4%e8%ae%b0%e5%bd%95&url=https%3a%2f%2fcgcel.github.io%2fposts%2f2022%2f05%2f17%2fscrapy%25E5%2588%259D%25E4%25B8%258A%25E6%2589%258B-%25E7%2588%25AC%25E5%258F%2596%25E5%25B0%258F%25E8%25AF%25B4%25E8%25AE%25B0%25E5%25BD%2595%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//yourDisqusShortname.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2022 <a href=https://cgcel.github.io>烂杯的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>