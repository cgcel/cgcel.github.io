<!DOCTYPE html>
<html><head>
<title>Scrapy初上手: 爬取小说记录</title>




<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="">
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
<meta content="telephone=no" name="format-detection">
<meta name="description" content="">
<meta name="renderer" content="webkit">
<meta name="theme-color" content="#ffffff">



<meta property="og:title" content="Scrapy初上手: 爬取小说记录" />
<meta property="og:description" content="开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.
准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用 pip 安装. 现在只需要一句 pip install scrapy 即可.
编译环境 编译环境当然还是永远的 VS Code.
实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.
那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:
根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cgcel.github.io/posts/2022/05/17/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/scrapy%E5%88%9D%E4%B8%8A%E6%89%8B-%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-17T14:13:36+00:00" />
<meta property="article:modified_time" content="2022-05-17T14:13:36+00:00" /><meta property="og:site_name" content="烂杯的博客" />






<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Scrapy初上手: 爬取小说记录"/>
<meta name="twitter:description" content="开篇 由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.
准备 安装 现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用 pip 安装. 现在只需要一句 pip install scrapy 即可.
编译环境 编译环境当然还是永远的 VS Code.
实现 计划 大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.
那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:
根据小说主页 URL 获取各章节标题以及 URL 根据章节 URL 爬取章节内容并保存 将 Scrapy 爬取到的内容按顺序排列后, 依次存入 ."/>











  




<link rel="icon" href="https://cgcel.github.io/images/avatar.jpg">



      <script src="/js/toc.js"></script>
    
    <link type="text/css" rel="stylesheet" href="/vendor/css/bootstrap.min.css">

<link rel="stylesheet" href="/scss/dark-mode.min.cb53f1bee2b8900cb4f082afbf00175d6618f281cf9a2fe8619e3b52d20b5721.css" integrity="sha256-y1PxvuK4kAy08IKvvwAXXWYY8oHPmi/oYZ47UtILVyE=" media="screen">


<link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Material+Icons">



















</head>
<body>
    	<div id="app"><div class="single-column-drawer-container" id="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            
            
            
                
                
                
                    
                
                
                
                <a class="a-block drawer-menu-item active" href="/posts">
                    Archive
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/categories">
                    Categories
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/tags">
                    Tags
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/index.xml">
                    RSS Feed
                </a>
                
            
            
            <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%bc%80%e7%af%87" class="nav-开篇">
									开篇
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%87%86%e5%a4%87" class="nav-准备">
									准备
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%ae%89%e8%a3%85" class="nav-安装">
									安装
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%bc%96%e8%af%91%e7%8e%af%e5%a2%83" class="nav-编译环境">
									编译环境
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e5%ae%9e%e7%8e%b0" class="nav-实现">
									实现
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e8%ae%a1%e5%88%92" class="nav-计划">
									计划
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%bc%80%e5%a7%8b" class="nav-开始">
									开始
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%8e%b7%e5%8f%96%e5%b0%8f%e8%af%b4%e7%ab%a0%e8%8a%82-url" class="nav-获取小说章节-url">
									获取小说章节 URL
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#items-%e7%9a%84%e4%bd%bf%e7%94%a8" class="nav-items-的使用">
									Items 的使用
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#item-pipeline-%e7%9a%84%e4%bd%bf%e7%94%a8" class="nav-item-pipeline-的使用">
									Item pipeline 的使用
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bc%a0%e9%80%92%e7%ab%a0%e8%8a%82%e5%8f%82%e6%95%b0-%e5%ae%9e%e7%8e%b0%e6%8c%89%e9%a1%ba%e5%ba%8f%e4%bf%9d%e5%ad%98" class="nav-传递章节参数-实现按顺序保存">
									传递章节参数, 实现按顺序保存
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#itemspy" class="nav-itemspy">
									items.py
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#pipelinespy" class="nav-pipelinespy">
									pipelines.py
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e8%bf%90%e8%a1%8c%e7%88%ac%e8%99%ab" class="nav-运行爬虫">
									运行爬虫
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%80%bb%e7%bb%93" class="nav-总结">
									总结
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
            
        </div>
    </div>
</div>
<transition name="fade">
    <div id="drawer-mask" v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav id="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div id="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a id="navTitle" class="navbar-brand" href="https://cgcel.github.io/">
            烂杯的博客
        </a>
        
        <button type="button" class="nav-darkmode-toggle" id="darkModeToggleButton2">
            <i class="material-icons" id="darkModeToggleIcon2">
                dark_mode
            </i>
        </button>
        
    </div>
</nav>
<div class="single-column-header-container" id="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="https://cgcel.github.io/">
        <div class="single-column-header-title">烂杯的博客</div>
        
        <div class="single-column-header-subtitle">我是小白, 随便写写.</div>
        

    </a>
</div>

            <div id="content">
                <div id="streamContainer" class="stream-container">

    <div class="post-list-container post-list-container-shadow">
        <div class="post">
            
            
            

            <div class="post-head-wrapper-text-only"
                
            >
                <div class="post-title">
                    Scrapy初上手: 爬取小说记录
                    
                    <div class="post-meta">
                        
                        <time itemprop="datePublished">
                            2022-05-17 14:13
                        </time>
                        

                        
                            <i class="material-icons" style="">folder</i>
                                <a href="/categories/">[学习]</a>
                                &nbsp;
                        

                        
                            <i class="material-icons" style="">label</i>
                            
                                <a href="/tags/python">python</a>
                                &nbsp;
                            
                                <a href="/tags/scrapy">scrapy</a>
                                &nbsp;
                            
                        
                        
                    </div>
                </div>
            </div>
            
            <div class="post-body-wrapper">
                
                <div class="post-body" v-pre>
                
                    <h2 id="开篇">开篇</h2>
<p>由于最近因为疫情居家健康监测, 还没复工, 我在家边学习边玩. 在家期间看了 ⌈86⌋ 第二季, 简单学了一下 Go, 还有捡回来 Scrapy 学习一下. 说到 Scrapy, 本科的时候就有学习过, 但是当时找的是B站上的中文教程, 感觉教的我一头雾水, 这次从官方文档开始下手, 结合 Google, 感觉一下子清晰明了了许多.</p>
<hr>
<h2 id="准备">准备</h2>
<h3 id="安装">安装</h3>
<p>现在的 Scrapy 安装简单了不少, 几年前在 Windows 端还不能直接用 pip 安装. 现在只需要一句 <code>pip install scrapy</code> 即可.</p>
<h3 id="编译环境">编译环境</h3>
<p>编译环境当然还是永远的 VS Code.</p>
<hr>
<h2 id="实现">实现</h2>
<h3 id="计划">计划</h3>
<p>大佬提供了一个塞满 URL 的 json 文件给我, 都是小说网站相关的, 既然要爬取小说网站, 就不可避免地要爬取同一本书的多个章节, 于是很容易联想到使用 Scrapy 来实现, 毕竟是一个成名已久的 Python 爬虫框架, 总比我简单粗暴用 requests 来爬好多了.</p>
<p>那么接下来就是解决以下几个问题, 最后实现整个爬取的功能:</p>
<ul>
<li>根据小说主页 URL 获取各章节标题以及 URL</li>
<li>根据章节 URL 爬取章节内容并保存</li>
<li>将 Scrapy 爬取到的内容按顺序排列后, 依次存入 <code>.txt</code> 文件中</li>
</ul>
<h3 id="开始">开始</h3>
<p>首先, 安装完 Scrapy 后, 按照官方文档, 直接在目录启动命令行, 捅过命令行生成项目:</p>
<pre tabindex="0"><code>$ scrapy startproject biquge_dl
</code></pre><p>生成后, 在 <code>spiders/</code> 子文件夹中新建 <code>fiction_spider.py</code>, 用于编写爬虫代码.</p>
<p>根据官方的 QuickStart 例程, 首先新建一个 class, 继承 <code>scrapy.Spider</code>, 后续只需要结合源码和文档, 就可以继续写下去了.</p>
<h3 id="获取小说章节-url">获取小说章节 URL</h3>
<p>首先在浏览器打开开发者工具对小说主页进行抓包, 可以看到该网站并没有用到一些 API, 于是准备采用 XPATH 来对章节 URL 进行爬取, 观察到无论多长篇幅的小说, 其章节都在一页中显示, 这无疑降低了部分难度, 拿来练手 scrapy 实在是再合适不过了.</p>
<p><img src="Pasted_image_20220524212212.png" alt=""></p>
<p>通过重写 <code>parse()</code>, 对小说主页 html 进行解析并抓取章节 URL.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">scrapy</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">class</span> <span style="color:#008b45;font-weight:bold">FictionSpider</span>(scrapy.Spider):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">parse</span>(self, response, **kwargs):
</span></span><span style="display:flex;"><span>        self.logger.info(<span style="color:#cd5555">&#39;Parse function called on </span><span style="color:#cd5555">%s</span><span style="color:#cd5555">&#39;</span>, response.url)
</span></span><span style="display:flex;"><span>        fiction_name = response.xpath(<span style="color:#cd5555">&#39;//*[@id=&#34;info&#34;]/h1/text()&#39;</span>).get()
</span></span><span style="display:flex;"><span>        section_order = <span style="color:#b452cd">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">for</span> episode_doc <span style="color:#8b008b">in</span> response.xpath(<span style="color:#cd5555">&#39;//*[@id=&#34;list&#34;]/dl/dd[*]/a&#39;</span>):
</span></span><span style="display:flex;"><span>            section_title = episode_doc.xpath(<span style="color:#cd5555">&#39;.//text()&#39;</span>).get()            
</span></span><span style="display:flex;"><span>            url = episode_doc.xpath(<span style="color:#cd5555">&#39;.//@href&#39;</span>).get()            
</span></span><span style="display:flex;"><span>            section_url = self.base_url + url            
</span></span><span style="display:flex;"><span>            request = scrapy.Request(
</span></span><span style="display:flex;"><span>                url=section_url, callback=self.parse_content, cb_kwargs=<span style="color:#658b00">dict</span>())
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>            request.cb_kwargs[<span style="color:#cd5555">&#39;fiction_name&#39;</span>] = fiction_name            
</span></span><span style="display:flex;"><span>            request.cb_kwargs[<span style="color:#cd5555">&#39;section_order&#39;</span>] = section_order            
</span></span><span style="display:flex;"><span>            request.cb_kwargs[<span style="color:#cd5555">&#39;section_title&#39;</span>] = section_title            
</span></span><span style="display:flex;"><span>            section_order += <span style="color:#b452cd">1</span>            
</span></span><span style="display:flex;"><span>            <span style="color:#8b008b;font-weight:bold">yield</span> request
</span></span></code></pre></div><p>获取成功后, 将小说名, 章节序号, 章节名作为参数传给 <code>parse_content()</code> 函数, 继而对章节进行爬取.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">bs4</span> <span style="color:#8b008b;font-weight:bold">import</span> BeautifulSoup <span style="color:#8b008b;font-weight:bold">as</span> bs
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">biquge_dl.items</span> <span style="color:#8b008b;font-weight:bold">import</span> BiqugeDlItem
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">class</span> <span style="color:#008b45;font-weight:bold">FictionSpider</span>(scrapy.Spider):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">parse_content</span>(self, response, fiction_name, section_order, section_title):   
</span></span><span style="display:flex;"><span>        self.logger.info(<span style="color:#cd5555">&#39;Parse content function called on </span><span style="color:#cd5555">%s</span><span style="color:#cd5555">&#39;</span>, response.url)       
</span></span><span style="display:flex;"><span>        contents = response.xpath(<span style="color:#cd5555">&#39;//*[@id=&#34;content&#34;]/text()&#39;</span>).getall()
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> <span style="color:#658b00">len</span>(contents) &lt;= <span style="color:#b452cd">1</span>:       
</span></span><span style="display:flex;"><span>            soup = bs(response.body, <span style="color:#cd5555">&#39;html.parser&#39;</span>)           
</span></span><span style="display:flex;"><span>            contents = soup.find_all(<span style="color:#cd5555">&#34;div&#34;</span>, {<span style="color:#cd5555">&#34;class&#34;</span>: <span style="color:#cd5555">&#34;box_con&#34;</span>}).get_text()
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>        item = BiqugeDlItem()        
</span></span><span style="display:flex;"><span>        item[<span style="color:#cd5555">&#39;name&#39;</span>] = fiction_name        
</span></span><span style="display:flex;"><span>        item[<span style="color:#cd5555">&#39;order&#39;</span>] = section_order        
</span></span><span style="display:flex;"><span>        item[<span style="color:#cd5555">&#39;title&#39;</span>] = section_title
</span></span><span style="display:flex;"><span>        item[<span style="color:#cd5555">&#39;content&#39;</span>] = contents
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">return</span> item
</span></span></code></pre></div><h3 id="items-的使用">Items 的使用</h3>
<p>写上述  <code>parse_content()</code> 时发现, 直接使用 scrapy 对小说进行章节爬取, 并按章节名每章下载是很简单的, 只需要直接跑爬虫就可以, 各个章节会乱序下载下来, 但如果想要只下载一份文件, 按顺序存储章节, 那么就需要使用 scrapy 的 <code>items</code> 和 <code>pipelines</code> 了, 这两个 .py 文件会随着一开始命令行创建项目时一并创建, 所以可以很轻易地在 <code>biquge_dl/</code> 文件夹内找到, 只需要对这两个文件进行重写, 并在 <code>parse_content()</code> 中将内容写进 item 中, 就能实现参数的传递.</p>
<p>查阅 <a href="%5Bhttps://docs.scrapy.org/en/latest/topics/items.html%5D(https://docs.scrapy.org/en/latest/topics/items.html#module-scrapy.item)">官方文档</a>, 对 Items 的描述如下:</p>
<p><em>The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#topics-spiders">Spiders</a> may return the extracted data as items, Python objects that define key-value pairs.</em></p>
<p><em>Scrapy supports <a href="https://docs.scrapy.org/en/latest/topics/items.html#item-types">multiple types of items</a>. When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should <a href="https://docs.scrapy.org/en/latest/topics/items.html#supporting-item-types">work for any item type</a>.</em></p>
<p>可见 item 可以将提取的数据按一定格式返回, 只需要规划好传入参数, 就可以利用 item 实现排序的操作.</p>
<h3 id="item-pipeline-的使用">Item pipeline 的使用</h3>
<p><a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html">官方文档</a> 这样介绍:</p>
<p><em>After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.</em></p>
<p><em>Each item pipeline component (sometimes referred as just “Item Pipeline”) is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.</em></p>
<p><em>Typical uses of item pipelines are:</em></p>
<ul>
<li><em>cleansing HTML data</em></li>
<li><em>validating scraped data (checking that the items contain certain fields)</em></li>
<li><em>checking for duplicates (and dropping them)</em></li>
<li><em>storing the scraped item in a database</em></li>
</ul>
<p>意为, 在 spider 抓取一个 item 后, scrapy 通过 Item Pipeline 来处理这个 item, 结合本项目, 只需要将爬取到的 item 交由 Item Pipeline 统一处理即可实现按顺序存储的功能.</p>
<h3 id="传递章节参数-实现按顺序保存">传递章节参数, 实现按顺序保存</h3>
<p>为了实现章节按照顺序写入, 首先确认实现思路:</p>
<ol>
<li>在 items.py 定义一个 Item 类, 自定义传入参数, 以供调用</li>
<li>在 pipelines.py 定义一个 Pipeline 类, 对传入的 item 进行处理</li>
</ol>
<h4 id="itemspy">items.py</h4>
<p>打开 items.py, 按照例子创建一个继承 <code>scrapy.Item</code> 的 <code>BiqugeDlItem</code> 类, 对类进行参数的定义:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">class</span> <span style="color:#008b45;font-weight:bold">BiqugeDlItem</span>(scrapy.Item):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># define the fields for your item here like:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># name = scrapy.Field()</span>
</span></span><span style="display:flex;"><span>    name = scrapy.Field()
</span></span><span style="display:flex;"><span>    order = scrapy.Field()
</span></span><span style="display:flex;"><span>    title = scrapy.Field()
</span></span><span style="display:flex;"><span>    content = scrapy.Field()
</span></span></code></pre></div><p>可见, 这里定义了几个参数, 分别是:</p>
<ul>
<li>name: 小说名</li>
<li>order: 章节序号</li>
<li>title: 章节标题</li>
<li>content: 章节内容</li>
</ul>
<p>此时再回看 <code>parse_content()</code> 函数, 函数实例化了一个 BiqugeDlItem 类, 并将小说名, 章节序号, 章节标题以及爬取得到的章节内容存入了 item 中, 最终将存有单章节数据的 item 对象返回.</p>
<h4 id="pipelinespy">pipelines.py</h4>
<p>打开 pipelines.py, 定义一个 <code>BiqugeDlPipeline</code> 类, 在类中重写 <code>open_spider(self, spider)</code>, <code>process_item(self, item, spider)</code> 以及 <code>close_spider(self, spider)</code> 函数, 根据文档介绍:</p>
<ul>
<li><code>open_spider(self, spider)</code> 函数在爬虫启动时被调用</li>
<li><code>process_item(self, item, spider)</code> 函数处理每一个返回的 item 对象</li>
<li><code>close_spider(self, spider)</code> 函数在爬虫运行结束后调用</li>
</ul>
<p>结合以上特点, 我们可以在 pipielines.py 中实现对包含小说名, 章节序号, 章节标题, 章节内容的 item 的处理, 代码如下:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">class</span> <span style="color:#008b45;font-weight:bold">BiqugeDlPipeline</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">open_spider</span>(self, spider):
</span></span><span style="display:flex;"><span>        self.items = []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">process_item</span>(self, item, spider):
</span></span><span style="display:flex;"><span>        self.items.append(item)
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">return</span> item
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">close_spider</span>(self, spider):
</span></span><span style="display:flex;"><span>        self.items.sort(key=<span style="color:#8b008b;font-weight:bold">lambda</span> i: i[<span style="color:#cd5555">&#39;order&#39;</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">for</span> item <span style="color:#8b008b">in</span> self.items:
</span></span><span style="display:flex;"><span>            <span style="color:#228b22"># print(item[&#39;title&#39;])</span>
</span></span><span style="display:flex;"><span>            <span style="color:#8b008b;font-weight:bold">with</span> <span style="color:#658b00">open</span>(<span style="color:#cd5555">&#39;downloads/</span><span style="color:#cd5555">{}</span><span style="color:#cd5555">.txt&#39;</span>.format(item[<span style="color:#cd5555">&#39;name&#39;</span>]), <span style="color:#cd5555">&#39;a&#39;</span>, encoding=<span style="color:#cd5555">&#39;utf-8&#39;</span>) <span style="color:#8b008b;font-weight:bold">as</span> f:
</span></span><span style="display:flex;"><span>                f.write(<span style="color:#cd5555">&#39;</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">{}</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">&#39;</span>.format(item[<span style="color:#cd5555">&#39;title&#39;</span>]))
</span></span><span style="display:flex;"><span>                <span style="color:#8b008b;font-weight:bold">for</span> content <span style="color:#8b008b">in</span> item[<span style="color:#cd5555">&#39;content&#39;</span>]:
</span></span><span style="display:flex;"><span>                    f.write(content.strip())
</span></span></code></pre></div><p>在 <code>open_spider()</code> 中, 先定义一个空列表, 用于存储各个 item, 在 <code>process_item()</code> 中, 将传入的 item 添加至 items 列表中, 在 <code>close_spider()</code> 中, 对 items 列表进行排序操作, 然后将列表中内容写入最终文件, 实现了小说的按序存储. 至此, 整个爬取流程结束.</p>
<hr>
<h2 id="运行爬虫">运行爬虫</h2>
<p>调试完成后, 在命令行启动爬虫, 测试爬取效果:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ scrapy crawl fiction_spider -a <span style="color:#00688b">url</span>=https://www.biduoxs.com/biquge/79_79244/
</span></span></code></pre></div><p><img src="Pasted_image_20220525175633.png" alt=""></p>
<hr>
<h2 id="总结">总结</h2>
<p>由于疫情管控在家荒废了一段时间, 不过好在还算有一点残存的自制力, 努力学习了一点东西, 这篇博客应该算是手敲的最长的一篇了, 虽然花的时间长了一点, 但是总归是学到了一些, 特别是结合源码和文档来边总结边学习, 能够领会到一些新知识. 这是对着例程敲代码所比较难学到的.</p>

                    
                    <HR width="100%" id="EOF">
		    <p style="color:#777;">Last modified on 2022-05-17</p>
                    
                </div>
            </div>
            
            
            <nav class="post-pagination">

                
                <a class="newer-posts" href="/posts/2022/06/07/pi4tntcase--%E6%88%91%E7%9A%84%E6%A0%91%E8%8E%93%E6%B4%BE%E6%9C%BA%E7%AE%B1/pi4tntcase--%E6%88%91%E7%9A%84%E6%A0%91%E8%8E%93%E6%B4%BE%E6%9C%BA%E7%AE%B1/">
			Next<br>Pi4TNTCase -- 我的树莓派机箱
                </a>
                
                
                
                <a class="older-posts" href="/posts/2022/04/13/%E4%B8%80%E5%8A%A08t%E5%88%B7nameless-aosp%E8%AE%B0%E5%BD%95/%E4%B8%80%E5%8A%A08t%E5%88%B7nameless-aosp%E8%AE%B0%E5%BD%95/">
			Previous<br>一加8T刷Nameless AOSP记录
                </a>
                
            </nav>
            <div class="post-comment-wrapper">
                












<script src="https://giscus.app/client.js"
        data-repo="username/repo"
        data-repo-id="**************************"
        data-category="General"
        data-category-id="*********************"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

            </div>
        </div>
    </div>


                    </div>
            </div><div id="sideContainer" class="side-container">
    
    <a class="a-block nav-head false" href="https://cgcel.github.io/">
    
        <div class="nav-title">
            烂杯的博客
        </div>
        
        <div class="nav-subtitle">
            我是小白, 随便写写.
        </div>
        
    </a>

    <div class="nav-link-list">
        
        
            
            
            
                
            
            
            
            <a class="a-block nav-link-item active" href="/posts">
                Archive
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/categories">
                Categories
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/tags">
                Tags
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/index.xml">
                RSS Feed
            </a>
            
        
    </div>

    

    <div class="nav-footer">
        
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	This is a customized copyright.
	

    </div>
    
</div><div id="extraContainer" class="extra-container">
    <div class="toc-wrapper">
        

        
        <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%bc%80%e7%af%87" class="nav-开篇">
									开篇
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%87%86%e5%a4%87" class="nav-准备">
									准备
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%ae%89%e8%a3%85" class="nav-安装">
									安装
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%bc%96%e8%af%91%e7%8e%af%e5%a2%83" class="nav-编译环境">
									编译环境
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e5%ae%9e%e7%8e%b0" class="nav-实现">
									实现
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e8%ae%a1%e5%88%92" class="nav-计划">
									计划
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%bc%80%e5%a7%8b" class="nav-开始">
									开始
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%8e%b7%e5%8f%96%e5%b0%8f%e8%af%b4%e7%ab%a0%e8%8a%82-url" class="nav-获取小说章节-url">
									获取小说章节 URL
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#items-%e7%9a%84%e4%bd%bf%e7%94%a8" class="nav-items-的使用">
									Items 的使用
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#item-pipeline-%e7%9a%84%e4%bd%bf%e7%94%a8" class="nav-item-pipeline-的使用">
									Item pipeline 的使用
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bc%a0%e9%80%92%e7%ab%a0%e8%8a%82%e5%8f%82%e6%95%b0-%e5%ae%9e%e7%8e%b0%e6%8c%89%e9%a1%ba%e5%ba%8f%e4%bf%9d%e5%ad%98" class="nav-传递章节参数-实现按顺序保存">
									传递章节参数, 实现按顺序保存
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#itemspy" class="nav-itemspy">
									items.py
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#pipelinespy" class="nav-pipelinespy">
									pipelines.py
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e8%bf%90%e8%a1%8c%e7%88%ac%e8%99%ab" class="nav-运行爬虫">
									运行爬虫
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%80%bb%e7%bb%93" class="nav-总结">
									总结
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
        
    </div>
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top"
            :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>
        
        <a type="button" class="pagination-action" id="darkModeToggleButton">
            <span class="material-icons pagination-action-icon" id="darkModeToggleIcon">
                dark_mode
            </span>
        </a>
        
        
    </div>
</div><div id="single-column-footer">
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	This is a customized copyright.
	
</div>
            </div>
    
    <script src="/js/journal.js"></script></body>
</html>
