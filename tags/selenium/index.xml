<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>selenium on 烂杯的博客</title>
        <link>https://cgcel.github.io/tags/selenium/</link>
        <description>Recent content in selenium on 烂杯的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 06 Jul 2020 21:43:54 +0800</lastBuildDate><atom:link href="https://cgcel.github.io/tags/selenium/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>最近开的一个爬虫小坑</title>
        <link>https://cgcel.github.io/posts/2020/07/06/2020-07-06-%E6%9C%80%E8%BF%91%E5%BC%80%E7%9A%84%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB%E5%B0%8F%E5%9D%91/</link>
        <pubDate>Mon, 06 Jul 2020 21:43:54 +0800</pubDate>
        
        <guid>https://cgcel.github.io/posts/2020/07/06/2020-07-06-%E6%9C%80%E8%BF%91%E5%BC%80%E7%9A%84%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB%E5%B0%8F%E5%9D%91/</guid>
        <description>&lt;h2 id=&#34;起因&#34;&gt;起因&lt;/h2&gt;
&lt;p&gt;去年年底开始, 部门里统计每月运维分队保障情况的任务就落到了我们头上, 大家都不是很愿意去做, 毕竟是重复劳动, 没有什么意义, 在信息系统里查起来还比较麻烦, 数据要一个一个地对, 然后根据当月分队的排班情况算到每个分队的统计里, 并按照规则排名以及打分, 一般都是每个月23或者24号开始统计, 正好这个月我 21-24 号放假加调休, 心想终于躲过一劫的我居然在回到公司上班的第一天被通知由我来统计! 无奈之下我只好以最快的速度做完, 但是这样下去什么时候才是个头啊, 于是就萌生了自己造轮子的想法, 正好复习一下自学的 python 爬虫.&lt;/p&gt;
&lt;h2 id=&#34;尝试&#34;&gt;尝试&lt;/h2&gt;
&lt;h3 id=&#34;requests&#34;&gt;requests&lt;/h3&gt;
&lt;p&gt;写些小爬虫的时候, 用 requests 是最方便的. 一般来说模拟登录成功的话也就成功了一半, 后续数据可以轻松获得. 但是信息系统的一些前端渲染的数据无法通过 requests 获得, 比如下面使用 BeautifulSoup 解析后没有数据:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;span&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;lblCount&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;color:#C00000;font-weight:bold;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;span&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;selenium&#34;&gt;selenium&lt;/h3&gt;
&lt;p&gt;于是我只能用 selenium 来写, 这也是我第一次全程用 selenium 实现功能的爬虫. selenium 的使用也非常简单, 思路就是使用 &lt;code&gt;find_element_by_id()&lt;/code&gt; 和 &lt;code&gt;find_element_by_xpath()&lt;/code&gt; 来操作浏览器点击或者输入信息, 将响应后的网页源码通过 BeautifulSoup 解析, 得到想要的数据, 如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;span&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;lblCount&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;color:#C00000;font-weight:bold;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;1&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;span&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;处理数据&#34;&gt;处理数据&lt;/h3&gt;
&lt;p&gt;有了数据之后还要解决一个问题, 也就是推出各个分队的排班, 以便将爬到的数据算到对应责任分队处, 我是用了建 dict 和 list 的方式, 将4天一周期的分队排班和时间段结合, 得出正确的排班, 同时也新建各种dict用于存放分队运行数据, 如延误行李数, 事前维修次数, 责任分区维修次数, 值班日志以及根据这些数据排序得到的分数等等.&lt;/p&gt;
&lt;h2 id=&#34;运行环境&#34;&gt;运行环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Windows 7-10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与本机 Chrome 版本对应的 &lt;code&gt;chromedriver&lt;/code&gt; (项目中版本为 &lt;code&gt;Chrome version 89&lt;/code&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://sites.google.com/a/chromium.org/chromedriver/downloads&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载链接1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://chromedriver.storage.googleapis.com/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载链接2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://npm.taobao.org/mirrors/chromedriver&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载链接3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Microsoft Office 或其他&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;工作原理&#34;&gt;工作原理&lt;/h2&gt;
&lt;h3 id=&#34;编译环境&#34;&gt;编译环境:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.8.6 (64-bit)&lt;/li&gt;
&lt;li&gt;Visual Studio Code&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;安装依赖&#34;&gt;安装依赖:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;selenium&lt;/li&gt;
&lt;li&gt;openpyxl&lt;/li&gt;
&lt;li&gt;bs4&lt;/li&gt;
&lt;li&gt;pyyaml&lt;/li&gt;
&lt;li&gt;tqdm&lt;/li&gt;
&lt;li&gt;lxml&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实现原理&#34;&gt;实现原理:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;通过 &lt;code&gt;selenium&lt;/code&gt; 调用 &lt;code&gt;chromedriver&lt;/code&gt; 打开 chrome, 打开信息系统登录页面并模拟登录&lt;/li&gt;
&lt;li&gt;登录成功后请求对应延误行李, 值班日志等页面, 使用 &lt;code&gt;BeautifulSoup&lt;/code&gt;, &lt;code&gt;re&lt;/code&gt; 爬取数据&lt;/li&gt;
&lt;li&gt;对数据进行处理后, 使用 &lt;code&gt;openpyxl&lt;/code&gt; 将数据写入 Excel 中&lt;/li&gt;
&lt;li&gt;功能调试完成后将 chrome 设置为 headless browser, 优化使用体验&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;使用方法&#34;&gt;使用方法&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;打开 &lt;code&gt;config.yaml&lt;/code&gt;, 按照指引修改 &lt;code&gt;账号&lt;/code&gt;, &lt;code&gt;密码&lt;/code&gt;, &lt;code&gt;起始日期&lt;/code&gt;, &lt;code&gt;结束日期&lt;/code&gt;, &lt;code&gt;刷新间隔&lt;/code&gt;, &lt;code&gt;航站楼&lt;/code&gt;, &lt;code&gt;内外网访问模式&lt;/code&gt;, &lt;code&gt;运行模式&lt;/code&gt;, 其中 &lt;code&gt;刷新间隔&lt;/code&gt; 可按需修改, 修改完保存配置&lt;/li&gt;
&lt;li&gt;运行 &lt;code&gt;main.exe&lt;/code&gt; 或 &lt;code&gt;main.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;程序窗口将显示爬取进度, 运行完毕后, &lt;code&gt;统计结果&lt;/code&gt; 以及 &lt;code&gt;分队排班表&lt;/code&gt; 以 Excel 格式保存在 &lt;code&gt;\output&lt;/code&gt; 目录中&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;后续&#34;&gt;后续&lt;/h2&gt;
&lt;p&gt;脚本我从 &lt;code&gt;v1.0&lt;/code&gt; 开始记录, 在使用过程中一直有遇到新的bug, 导致统计数字有问题, 期间也是一直有维护, 修了几个bug, 直到最近更新到了 &lt;code&gt;v2.6.3&lt;/code&gt;, 相信统计功能已经接近完美 (flag)&lt;/p&gt;
&lt;p&gt;不过也是多亏了同事一直有帮忙检验表格准确性, 这些 bug 才得以被修复, 如果是我自己用估计也不会发现.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
